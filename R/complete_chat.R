#' Complete an LLM Chat
#'
#' @description
#' Submits a prompt to OpenAI's "Chat" API endpoint and formats the response into a string or tidy dataframe.
#'
#'
#' @param prompt The prompt
#' @param model  Which OpenAI model to use. Defaults to 'gpt-3.5-turbo'
#' @param openai_api_key Your API key. By default, looks for a system environment variable called "OPENAI_API_KEY" (recommended option). Otherwise, it will prompt you to enter the API key as an argument.
#' @param max_tokens How many tokens (roughly 4 characters of text) should the model return? Defaults to a single token (next word prediction).
#' @param temperature A numeric between 0 and 2 When set to zero, the model will always return the most probable next token. For values greater than zero, the model selects the next word probabilistically.
#'
#' @return If max_tokens = 1, returns a dataframe with the 5 most likely next-word responses and their probabilities. If max_tokens > 1, returns a single string of text generated by the model.
#' @export
#'
#' @examples
#' format_chat('Are frogs sentient? Yes or No.') |> complete_chat()
#' format_chat('Write a haiku about frogs.') |> complete_chat(max_tokens = 100)
complete_chat <- function(prompt,
                          model = 'gpt-3.5-turbo',
                          openai_api_key = Sys.getenv('OPENAI_API_KEY'),
                          max_tokens = 1,
                          temperature = 1) {

  if(openai_api_key == ''){
    stop("No API key detected in system environment. You can enter it manually using the 'openai_api_key' argument.")
  }


  # code adapted from https://github.com/irudnyts/openai

  ## Build path parameters ----------------------

  task <- "chat/completions"

  base_url <- glue::glue("https://api.openai.com/v1/{task}")

  headers <- c(
    "Authorization" = paste("Bearer", openai_api_key),
    "Content-Type" = "application/json"
  )

  ## Build request body ----------------------------

  body <- list()
  body[['model']] <- model
  body[['messages']] <- prompt
  body[['max_tokens']] <- max_tokens
  body[['temperature']] <- temperature
  if(max_tokens == 1){
    body[['logprobs']] <- TRUE
    body[['top_logprobs']] <- 5
  }

  ## Make a request and parse it ----------------
  response <- httr::POST(
    url = base_url,
    httr::add_headers(.headers = headers),
    body = body,
    encode = "json"
  )

  parsed <- response |>
    httr::content(as = "text", encoding = "UTF-8") |>
    jsonlite::fromJSON(flatten = TRUE)

  ## Check whether request failed and return parsed --------------

  if (httr::http_error(response)) {
    paste0(
      "OpenAI API request failed [",
      httr::status_code(response),
      "]:\n\n",
      parsed$error$message
    ) |>
      stop(call. = FALSE)
  }

  # if max_tokens > 1, return the text
  to_return <- parsed$choices$message.content

  # if max_tokens == 1, return a tidy dataframe of probabilities for each prompt
  if(max_tokens == 1){

    df <- parsed$choices$logprobs.content[[1]]$top_logprobs[[1]]

    df$probability <- exp(df$logprob)

    to_return <- data.frame(token = df$token,
                            probability = df$probability)

  }

  return(to_return)

}
