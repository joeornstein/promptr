#' Complete an LLM Prompt
#'
#' @description
#' Submits a text prompt to OpenAI's "Completion" API endpoint and formats the response into a string or tidy dataframe. (Note that, as of 2024, this endpoint is considered "Legacy" by OpenAI and is likely to be deprecated.)
#'
#'
#' @param prompt The prompt
#' @param model  Which OpenAI model to use. Defaults to 'davinci-002'
#' @param openai_api_key Your API key. By default, looks for a system environment variable called "OPENAI_API_KEY" (recommended option). Otherwise, it will prompt you to enter the API key as an argument.
#' @param max_tokens How many tokens (roughly 4 characters of text) should the model return? Defaults to a single token (next word prediction).
#' @param temperature A numeric between 0 and 2 When set to zero, the model will always return the most probable next token. For values greater than zero, the model selects the next word probabilistically.
#'
#' @return If max_tokens = 1, returns a dataframe with the 5 most likely next words and their probabilities. If max_tokens > 1, returns a single string of text generated by the model.
#' @export
#'
#' @examples
#' complete_prompt('I feel like a')
#' complete_prompt('Here is my haiku about frogs:',
#'                 max_tokens = 100)
complete_prompt <- function(prompt,
                            model = 'gpt-3.5-turbo-instruct',
                            openai_api_key = Sys.getenv('OPENAI_API_KEY'),
                            max_tokens = 1,
                            temperature = 0) {

  if(openai_api_key == ''){
    stop("No API key detected in system environment. You can enter it manually using the 'openai_api_key' argument.")
  }


  # code adapted from https://github.com/irudnyts/openai

  ## Build path parameters ----------------------

  task <- "completions"

  base_url <- glue::glue("https://api.openai.com/v1/{task}")

  headers <- c(
    "Authorization" = paste("Bearer", openai_api_key),
    "Content-Type" = "application/json"
  )

  ## Build request body ----------------------------

  body <- list()
  body[['model']] <- model
  body[['prompt']] <- prompt
  body[['max_tokens']] <- max_tokens
  body[['temperature']] <- temperature
  if(max_tokens == 1){
    body[['logprobs']] <- 5
  } else{
    body[['logprobs']] <- NULL
  }

  ## Make a request and parse it ----------------
  response <- httr::POST(
    url = base_url,
    httr::add_headers(.headers = headers),
    body = body,
    encode = "json"
  )

  parsed <- response |>
    httr::content(as = "text", encoding = "UTF-8") |>
    jsonlite::fromJSON(flatten = TRUE)

  ## Check whether request failed and return parsed --------------

  if (httr::http_error(response)) {
    paste0(
      "OpenAI API request failed [",
      httr::status_code(response),
      "]:\n\n",
      parsed$error$message
    ) |>
      stop(call. = FALSE)
  }

  # if max_tokens > 1, return the text
  to_return <- parsed$choices$text

  # if max_tokens == 1, return a tidy dataframe of probabilities
  if(max_tokens == 1){
    tokens <- names(parsed$choices$logprobs.top_logprobs[[1]])
    logprobs <- as.numeric(parsed$choices$logprobs.top_logprobs[[1]])

    to_return <- data.frame(token = tokens,
                           probability = exp(logprobs))
  }

  return(to_return)

}
